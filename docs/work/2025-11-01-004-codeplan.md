Here is a detailed, phased code plan for building the `beads` CLI in Rust, starting with the minimum viable features you can test.

### Minimum 4 Features for a Testable MVP (Phase 1)

To test the core "Log is Truth, DB is Cache" architecture, you don't need Git or networking. You just need to prove the local write-and-read loop.

Here are the minimum 4 features for a testable "Phase 1":

1.  **`beads init`**: This is the prerequisite. It creates the `.beads` directory, the `events.jsonl` log file, and the `beads.db` cache with the correct schema. This sets the stage.
2.  **`beads create --title "..."`**: This is your "Write" command. It must perform the full "Workflow 4.3":
      * Generate a new monotonic ULID.
      * Append the `create` event to `events.jsonl`.
      * *Immediately* apply that same event to the `beads.db` cache.
      * Update the `_meta` table (offset/event\_id).
3.  **`beads list`**: This is your "Read" command. It must read *only* from the `beads.db` cache. If this shows the issue you just created, you've proven that the `create` command correctly updated the cache.
4.  **`beads sync --full`**: This is the "Recovery" command and the *most important test*. After creating a few issues, you will **manually delete `beads.db`**. Then, running `beads sync --full` will force the CLI to read `events.jsonl` from the beginning, replay all events, and *re-build* `beads.db`. Running `beads list` again will prove that the log is the true source of truth.

This MVP fully tests the local-first, event-sourcing model, which is the foundation for everything else.

-----

### Rust CLI Code Plan

Here is a phase-by-phase plan to build the CLI.

#### Project Setup

1.  **Create Project:**
    ```bash
    cargo new beads
    cd beads
    ```
2.  **Add Dependencies (`Cargo.toml`):**
    ```toml
    [dependencies]
    clap = { version = "4.5", features = ["derive"] }
    rusqlite = { version = "0.31", features = ["bundled"] }
    serde = { version = "1.0", features = ["derive"] }
    serde_json = "1.0"
    ulid = "1.1" # For event_id
    chrono = "0.4" # For ts
    thiserror = "1.0"
    anyhow = "1.0"
    ```

#### Phase 0: Core Data Structures (The "Model")

Create `src/model.rs` and `src/error.rs` to define the core types.

```rust 
// error.rs 
use thiserror::Error;

#[derive(Error, Debug)]
pub enum BeadsError {
    #[error("Database error: {0}")]
    Db(#[from] rusqlite::Error),
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    #[error("JSON serialization error: {0}")]
    Json(#[from] serde_json::Error),
    #[error("Initialization failed: .beads directory already exists")]
    AlreadyInitialized,
    #[error("Not a beads repository (or parent): .beads directory not found")]
    NotARepository,
    #[error("ULID generation error: {0}")]
    Ulid(#[from] ulid::MonotonicError),
    #[error("Event log corrupted: {0}")]
    LogCorrupted(String),
}

pub type Result<T> = std::result::Result<T, BeadsError>;

```

```rust 
// model.rs 
use serde::{Deserialize, Serialize};

// --- Event Model (for events.jsonl) ---

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Event {
    pub event_id: String, // ULID
    pub ts: String,       // ISO 8601
    pub op: OpKind,
    pub id: String,       // Issue ID (e.g., "bd-1")
    pub actor: String,
    pub data: EventData,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
#[serde(rename_all = "snake_case")]
pub enum OpKind {
    Create,
    Update,
    Link,
    Unlink,
    Comment,
    Archive,
}

#[derive(Serialize, Deserialize, Debug, Clone)]
#[serde(rename_all = "snake_case")]
pub enum EventData {
    Create {
        title: String,
        kind: String, // e.g., "task", "epic", "bug"
        priority: u32,
    },
    Update {
        // All fields are optional for partial updates
        title: Option<String>,
        status: Option<String>,
        assignee: Option<String>,
        description: Option<String>,
        design: Option<String>,
        // ... other fields
    },
    // Other ops to be added in later phases
    Link { to_id: String, kind: String },
    Unlink { to_id: String, kind: String },
    Comment { body: String },
    Archive { archived: bool },
}

// --- State Model (for beads.db) ---

#[derive(Debug)]
pub struct Issue {
    pub id: String,
    pub title: String,
    pub status: String,
    pub kind: String,
    pub priority: u32,
    pub created_at: String,
    // ... other fields
}

```

```rust 
// main.rs
use clap::{Parser, Subcommand};
use anyhow::Result;

// Define modules for each command/logic area
mod commands;
mod db;
mod error;
mod log;
mod model;
mod repo;

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
#[command(propagate_version = true)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Initialize a new beads repository
    Init,
    
    /// Create a new issue
    Create {
        #[arg(short, long)]
        title: String,
        
        #[arg(short, long, default_value = "task")]
        kind: String,
        
        #[arg(short, long, default_value_t = 2)]
        priority: u32,
    },
    
    /// List all issues
    List,
    
    /// Show details for a single issue
    Show { id: String },
    
    /// Apply new events from the log to the local database
    Sync {
        /// Force a full rebuild of the database from the event log
        #[arg(long)]
        full: bool,
    },
    
    // --- Phase 2+ Commands ---
    /// Update an existing issue
    Update {
        id: String,
        #[arg(short, long)]
        status: Option<String>,
        #[arg(long)]
        title: Option<String>,
        #[arg(long)]
        assignee: Option<String>,
    },
    
    /// Edit an issue's long-form spec in $EDITOR
    SpecEdit { id: String },
}

fn main() -> Result<()> {
    let cli = Cli::parse();
    
    // Find the repo root (.beads dir) before running any command
    // (except `init`)
    let repo = if !matches!(cli.command, Commands::Init) {
        Some(repo::find_repo()?)
    } else {
        None
    };

    match &cli.command {
        Commands::Init => {
            commands::init::run_init()?
        },
        Commands::Create { title, kind, priority } => {
            commands::create::run_create(repo.unwrap(), title, kind, *priority)?
        },
        Commands::List => {
            commands::list::run_list(repo.unwrap())?
        },
        Commands::Show { id } => {
            commands::show::run_show(repo.unwrap(), id)?
        },
        Commands::Sync { full } => {
            commands::sync::run_sync(repo.unwrap(), *full)?
        },
        CommandsAdditionalCommands => {
            // Handle Phase 2+ commands
            println!("This command will be implemented in a later phase.");
        }
    }

    Ok(())
}

```

 
#### `src/main.rs`: The CLI Entrypoint

This file will define the `clap` structure and delegate all logic to other modules.


-----

#### Phase 1: Local-First MVP

We'll create the modules for `repo`, `db`, `log`, and the Phase 1 commands.



```rust 
use crate::error::{BeadsError, Result};
use std::path::{Path, PathBuf};
use std::env;

pub const BEADS_DIR: &str = ".beads";
pub const LOG_FILE: &str = "events.jsonl";
pub const DB_FILE: &str = "beads.db";

/// Represents the set of paths for a beads repository
#[derive(Debug, Clone)]
pub struct BeadsRepo {
    pub root: PathBuf,
    pub beads_dir: PathBuf,
    pub log_path: PathBuf,
    pub db_path: PathBuf,
}

impl BeadsRepo {
    pub fn new(root: PathBuf) -> Self {
        let beads_dir = root.join(BEADS_DIR);
        let log_path = beads_dir.join(LOG_FILE);
        let db_path = beads_dir.join(DB_FILE);
        Self { root, beads_dir, log_path, db_path }
    }

    /// Open a connection to the SQLite database
    pub fn open_db(&self) -> Result<rusqlite::Connection> {
        Ok(rusqlite::Connection::open(&self.db_path)?)
    }
}

/// Find the beads repository root by searching up from the current dir
pub fn find_repo() -> Result<BeadsRepo> {
    let current_dir = env::current_dir()?;
    let mut current = current_dir.as_path();

    loop {
        let beads_dir = current.join(BEADS_DIR);
        if beads_dir.exists() && beads_dir.is_dir() {
            return Ok(BeadsRepo::new(current.to_path_buf()));
        }

        match current.parent() {
            Some(parent) => current = parent,
            None => return Err(BeadsError::NotARepository),
        }
    }
}

```

```rust 
// db.rs
use crate::error::{Result, BeadsError};
use crate::model::{Event, OpKind, EventData, Issue};
use crate::repo::BeadsRepo;
use rusqlite::{Connection, params};

/// Creates the initial database schema
pub fn create_schema(conn: &Connection) -> Result<()> {
    conn.execute_batch(
        "BEGIN;
        CREATE TABLE issues (
            id TEXT PRIMARY KEY,
            title TEXT NOT NULL,
            status TEXT NOT NULL,
            kind TEXT NOT NULL,
            priority INTEGER NOT NULL,
            created_at TEXT NOT NULL
            -- More fields like 'description', 'assignee' added here
        );
        CREATE TABLE dependencies (
            issue_id TEXT NOT NULL,
            depends_on_id TEXT NOT NULL,
            kind TEXT NOT NULL,
            PRIMARY KEY (issue_id, depends_on_id)
        );
        CREATE TABLE _meta (
            key TEXT PRIMARY KEY,
            value TEXT NOT NULL
        );
        INSERT INTO _meta (key, value) VALUES ('last_processed_offset', '0');
        INSERT INTO _meta (key, value) VALUES ('last_event_id', '0');
        COMMIT;",
    )?;
    Ok(())
}

/// Get a value from the _meta table
pub fn get_meta(conn: &Connection, key: &str) -> Result<String> {
    let val = conn.query_row(
        "SELECT value FROM _meta WHERE key = ?1",
        params![key],
        |row| row.get(0),
    )?;
    Ok(val)
}

/// Set a value in the _meta table
pub fn set_meta(conn: &Connection, key: &str, value: &str) -> Result<()> {
    conn.execute(
        "UPDATE _meta SET value = ?1 WHERE key = ?2",
        params![value, key],
    )?;
    Ok(())
}

/// The core logic for applying a single event to the database
pub fn apply_event(conn: &Connection, event: &Event) -> Result<()> {
    match &event.op {
        OpKind::Create => {
            if let EventData::Create { title, kind, priority } = &event.data {
                conn.execute(
                    "INSERT INTO issues (id, title, status, kind, priority, created_at)
                     VALUES (?1, ?2, 'open', ?3, ?4, ?5)",
                    params![event.id, title, kind, priority, event.ts],
                )?;
            }
        }
        OpKind::Update => {
            if let EventData::Update { title, status, .. } = &event.data {
                // This is a simple example. A real one would build the
                // SQL query dynamically based on which fields are Some.
                if let Some(s) = status {
                    conn.execute(
                        "UPDATE issues SET status = ?1 WHERE id = ?2",
                        params![s, event.id],
                    )?;
                }
                if let Some(t) = title {
                     conn.execute(
                        "UPDATE issues SET title = ?1 WHERE id = ?2",
                        params![t, event.id],
                    )?;
                }
            }
        }
        // Other ops (Link, Comment, etc.) would be handled here
        _ => {}
    }
    Ok(())
}

/// Fetches all issues from the cache
pub fn get_all_issues(conn: &Connection) -> Result<Vec<Issue>> {
    let mut stmt = conn.prepare("SELECT id, title, status, kind, priority, created_at FROM issues")?;
    let issue_iter = stmt.query_map([], |row| {
        Ok(Issue {
            id: row.get(0)?,
            title: row.get(1)?,
            status: row.get(2)?,
            kind: row.get(3)?,
            priority: row.get(4)?,
            created_at: row.get(5)?,
        })
    })?;

    let mut issues = Vec::new();
    for issue in issue_iter {
        issues.push(issue?);
    }
    Ok(issues)
}


```



```rust 
use crate::error::{Result, BeadsError};
use crate::model::{Event, OpKind, EventData};
use crate::repo::BeadsRepo;
use std::fs::{File, OpenOptions};
use std::io::{Write, Seek, SeekFrom};
use ulid::{Ulid, MonotonicGenerator};

/// The core "Write Event" workflow from Spec 4.3
pub fn write_event(
    repo: &BeadsRepo,
    conn: &rusqlite::Connection,
    mut generator: &mut MonotonicGenerator,
    op: OpKind,
    id: String,
    data: EventData,
) -> Result<Event> {
    
    // 1. Generate new ULID
    let event_id = generator.generate()?.to_string();
    let ts = chrono::Utc::now().to_rfc3339();
    let actor = "user@host".to_string(); // Placeholder

    let event = Event { event_id, ts, op, id, actor, data };

    // 2. Step 1 (Write to Log)
    let mut file = OpenOptions::new().append(true).open(&repo.log_path)?;
    
    // Get offset *before* writing
    let offset = file.seek(SeekFrom::End(0))?;
    
    let json_line = serde_json::to_string(&event)?;
    writeln!(file, "{}", json_line)?;

    // 3. Step 2 (Update Cache)
    // We apply this event immediately
    crate::db::apply_event(conn, &event)?;

    // 4. Step 3 (Update Meta)
    let tx = conn.transaction()?;
    crate::db::set_meta(&tx, "last_processed_offset", &offset.to_string())?;
    crate::db::set_meta(&tx, "last_event_id", &event.event_id)?;
    tx.commit()?;
    
    Ok(event)
}

/// Helper to get a seeded monotonic generator
pub fn get_monotonic_generator(conn: &rusqlite::Connection) -> Result<MonotonicGenerator> {
    let last_event_id_str = crate::db::get_meta(conn, "last_event_id")?;
    let last_ulid = Ulid::from_string(&last_event_id_str)
        .unwrap_or_else(|_| Ulid::from_datetime(std::time::SystemTime::UNIX_EPOCH));
    
    Ok(MonotonicGenerator::new_with_last(last_ulid))
}


```
http://googleusercontent.com/immersive_entry_chip/5

```rust 
use crate::error::{Result, BeadsError};
use crate::repo::{BeadsRepo, BEADS_DIR};
use crate::db;
use std::fs;
use std::env;

pub fn run_init() -> Result<()> {
    let current_dir = env::current_dir()?;
    let repo = BeadsRepo::new(current_dir);

    if repo.beads_dir.exists() {
        return Err(BeadsError::AlreadyInitialized);
    }

    // 1. Create .beads directory
    fs::create_dir(&repo.beads_dir)?;

    // 2. Create empty events.jsonl
    fs::File::create(&repo.log_path)?;

    // 3. Create beads.db and init schema
    let conn = repo.open_db()?;
    db::create_schema(&conn)?;

    println!("Initialized empty beads repository in {:?}", repo.beads_dir);
    println!("\nNext steps (for Phase 3 - Git sync):");
    println!("1. Add this to your .gitattributes file:");
    println!("   *.jsonl merge=beadslog");
    println!("2. Run 'beads --help' to see how to configure the merge driver.");
    Ok(())
}

```
http://googleusercontent.com/immersive_entry_chip/6

```rust 
use crate::error::Result;
use crate::repo::BeadsRepo;
use crate::model::{OpKind, EventData};
use crate::log;

pub fn run_create(repo: BeadsRepo, title: &str, kind: &str, priority: u32) -> Result<()> {
    let conn = repo.open_db()?;
    let mut generator = log::get_monotonic_generator(&conn)?;

    // TODO: We need a way to generate *new* issue IDs, e.g., "bd-1", "bd-2"
    // For now, use a ULID as the issue ID itself.
    let issue_id = ulid::Ulid::new().to_string();

    let op = OpKind::Create;
    let data = EventData::Create {
        title: title.to_string(),
        kind: kind.to_string(),
        priority,
    };

    let event = log::write_event(&repo, &conn, &mut generator, op, issue_id.clone(), data)?;

    println!("Created issue {}", event.id);
    Ok(())
}

```
http://googleusercontent.com/immersive_entry_chip/7

```rust 
use crate::error::Result;
use crate::repo::BeadsRepo;
use crate::db;

pub fn run_list(repo: BeadsRepo) -> Result<()> {
    let conn = repo.open_db()?;
    let issues = db::get_all_issues(&conn)?;

    if issues.is_empty() {
        println!("No issues found. Try `beads create`!");
        return Ok(());
    }

    // Simple text-based table
    println!("{:<12} | {:<8} | {:<40}", "ID", "STATUS", "TITLE");
    println!("{:-<12}-+-{:-<8}-+-{:-<40}", "", "", "");

    for issue in issues {
        let title = issue.title.chars().take(40).collect::<String>();
        println!("{:<12} | {:<8} | {:<40}", &issue.id[0..12], issue.status, title);
    }

    Ok(())
}

```
http://googleusercontent.com/immersive_entry_chip/8

```rust 
// (Similar to `list`, but queries for one ID and prints all fields)
// This is left as an exercise but follows the same pattern as `run_list`.
// 1. Open DB
// 2. `conn.query_row("SELECT * FROM issues WHERE id = ?1", ...)`
// 3. Map to `Issue` struct
// 4. Print formatted output

```
http://googleusercontent.com/immersive_entry_chip/9

```rust 
use crate::error::{Result, BeadsError};
use crate::repo::BeadsRepo;
use crate::db;
use crate::model::Event;
use std::fs::File;
use std::io::{BufReader, BufRead, Seek, SeekFrom};
use rusqlite::Transaction;

pub fn run_sync(repo: BeadsRepo, full: bool) -> Result<()> {
    if full {
        println!("Performing full sync, rebuilding database from log...");
        let conn = rebuild_db_from_log(&repo)?;
        println!("Database rebuilt successfully.");
        // We'd also update meta here
    } else {
        println!("Performing incremental sync...");
        let conn = repo.open_db()?;
        let (processed_count, new_offset, new_event_id) = 
            apply_new_events(&conn, &repo)?;
            
        // Update meta in a transaction
        let tx = conn.transaction()?;
        db::set_meta(&tx, "last_processed_offset", &new_offset.to_string())?;
        db::set_meta(&tx, "last_event_id", &new_event_id)?;
        tx.commit()?;
        
        println!("Applied {} new events.", processed_count);
    }
    Ok(())
}

/// The "full" sync logic
fn rebuild_db_from_log(repo: &BeadsRepo) -> Result<rusqlite::Connection> {
    // 1. Nuke and recreate DB
    std::fs::remove_file(&repo.db_path)?;
    let conn = repo.open_db()?;
    db::create_schema(&conn)?;

    // 2. Apply all events
    let (count, new_offset, new_event_id) = apply_new_events(&conn, repo)?;
    
    // 3. Set meta
    let tx = conn.transaction()?;
    db::set_meta(&tx, "last_processed_offset", &new_offset.to_string())?;
    db::set_meta(&tx, "last_event_id", &new_event_id)?;
    tx.commit()?;

    println!("Replayed {} events.", count);
    Ok(conn)
}

/// This is the core "Incremental Read" and "Reconcile" logic
/// from Workflow 4.2
fn apply_new_events(
    conn: &rusqlite::Connection,
    repo: &BeadsRepo,
) -> Result<(usize, u64, String)> {
    
    let last_offset = db::get_meta(conn, "last_processed_offset")?
        .parse::<u64>()
        .unwrap_or(0);
    let mut last_event_id = db::get_meta(conn, "last_event_id")?;

    let file = File::open(&repo.log_path)?;
    let mut reader = BufReader::new(file);
    
    // 1. Seek to last known offset
    reader.seek(SeekFrom::Start(last_offset))?;

    let mut new_lines = Vec::new();
    for line in reader.by_ref().lines() {
        new_lines.push(line?);
    }
    
    if new_lines.is_empty() {
        return Ok((0, last_offset, last_event_id));
    }
    
    // 2. Parse and Sort
    let mut new_events: Vec<Event> = Vec::new();
    for line in new_lines {
        if line.trim().is_empty() { continue; }
        let event: Event = serde_json::from_str(&line)
            .map_err(|e| BeadsError::LogCorrupted(format!("Failed to parse: {e}")))?;
        new_events.push(event);
    }
    
    // This is the LWW key: sort by ULID
    new_events.sort_by(|a, b| a.event_id.cmp(&b.event_id));

    // 3. Apply in transaction
    let tx = conn.transaction()?;
    for event in &new_events {
        // Guardrail from File 1
        if event.event_id <= last_event_id {
            // This is a complex recovery scenario.
            // For now, we just warn. A real implementation
            // might need to trigger a full rebuild.
            eprintln!(
                "Warning: Out-of-order event detected! {} <= {}",
                event.event_id,
                last_event_id
            );
        }
        
        db::apply_event(&tx, event)?;
        
        if event.event_id > last_event_id {
            last_event_id = event.event_id.clone();
        }
    }
    tx.commit()?;

    let new_offset = reader.seek(SeekFrom::Current(0))?;
    Ok((new_events.len(), new_offset, last_event_id))
}

```
http://googleusercontent.com/immersive_entry_chip/10

-----

#### Phase 2: Human-Friendly Workflow (`spec edit`)

  * **`beads update`**:
      * Implement the `Update` command in `main.rs`.
      * Create `src/commands/update.rs`.
      * This will be very similar to `create.rs`. It will:
        1.  Open DB, get a monotonic generator.
        2.  Build the `EventData::Update` struct (with `Some` for fields from CLI, `None` for others).
        3.  Call `log::write_event(...)` with `OpKind::Update`.
  * **`beads spec edit`**:
      * Implement the `SpecEdit` command.
      * Create `src/commands/spec_edit.rs`.
      * This is the most complex *human* workflow:
        1.  Needs a new `db::get_issue_by_id(&conn, id)` function.
        2.  Use `tempfile` crate to create `temp.md`.
        3.  Format the issue's `description`, `design`, etc., into the Markdown template and write to the temp file.
        4.  Get `$EDITOR` env var (or fallback to "vim"/"nano").
        5.  Use `std::process::Command` to open the editor. `command.status()?` to wait for it to close.
        6.  Read the file back.
        7.  Write a simple parser (e.g., regex on `## DESCRIPTION`) to extract the fields.
        8.  Build an `EventData::Update` struct.
        9.  Call `log::write_event(...)` just like `update` does.

#### Phase 3: The Multi-User Sync (The Hard Part)

  * **`beads-merge-driver`**:
      * Create a *new binary* in your project: `cargo new beads-merge-driver`.
      * This will be a very simple `main.rs` that takes 3 args from `env::args()`.
      * It will use a `std::collections::HashSet<String>` to read all lines from all 3 files (`BASE`, `LOCAL`, `REMOTE`) and insert them into the set.
      * Finally, it will iterate the set and `println!()` each line to stdout.
      * `beads init` will be updated to instruct users to build this (`cargo build --release`) and point their Git config to the binary in `target/release/beads-merge-driver`.
  * **`beads sync` (Update)**:
      * Update `src/commands/sync.rs`.
      * Add `std::process::Command` logic.
      * **Step 1:** Run `git pull`. If it fails, stop and report the Git error.
      * **Step 2:** Run the *existing* `apply_new_events()` logic (from Phase 1).
      * **Step 3:** Run `git add .beads/events.jsonl`.
      * **Step 4:** Run `git commit -m "Beads sync"` (only if there are changes).
      * **Step 5:** Run `git push`.
      * This composes Git and your local sync logic.

#### Phase 4: Remaining Features & Polish

  * Fill out `db::apply_event` for `Link`, `Comment`, `Archive`.
  * Implement the `beads dep add/remove` and `beads comment` commands (they are just new `write_event` calls).
  * Implement `beads ready` (a complex SQL query in `db.rs`).
  * Implement `beads compact`.